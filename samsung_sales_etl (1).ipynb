{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ðŸ“± Samsung Sales Analytics Pipeline (PySpark ETL)\n",
        "\n",
        "This notebook contains an end-to-end PySpark ETL pipeline that:\n",
        "- Ingests mock sales transactions, product catalog, and store/region data\n",
        "- Cleans and standardizes fields\n",
        "- Joins data to enrich transactions\n",
        "- Calculates KPIs: **revenue and units sold per model and country**\n",
        "- Writes curated outputs in **Parquet** format\n",
        "\n",
        "You can run this in Databricks, EMR, or any Spark environment with PySpark.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1) Configuration\n",
        "\n",
        "Update these paths to match your environment (DBFS/S3/ADLS/local)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Config ----\n",
        "INPUT_SALES = \"/data/raw/sales_transactions\"         # e.g., \"dbfs:/mnt/raw/sales_transactions\" or \"s3://bucket/raw/sales_transactions\"\n",
        "INPUT_PRODUCTS = \"/data/raw/product_catalog\"        # e.g., \"dbfs:/mnt/raw/product_catalog\"\n",
        "INPUT_STORES = \"/data/raw/store_regions\"            # e.g., \"dbfs:/mnt/raw/store_regions\"\n",
        "\n",
        "OUT_ENRICHED = \"/data/curated/enriched_transactions\" # enriched fact-like table\n",
        "OUT_KPI_MODEL_COUNTRY = \"/data/marts/kpi_model_country\"\n",
        "OUT_KPI_SUMMARY = \"/data/marts/kpi_summary\"\n",
        "\n",
        "WRITE_MODE = \"overwrite\"  # change to \"append\" for incremental strategies\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 2) Imports & Spark Session\n",
        "\n",
        "In Databricks, a Spark session usually exists as `spark`. In other environments, we create it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from pyspark.sql import SparkSession, DataFrame\n",
        "from pyspark.sql import functions as F\n",
        "from pyspark.sql import types as T\n",
        "\n",
        "try:\n",
        "    spark\n",
        "except NameError:\n",
        "    spark = SparkSession.builder.appName(\"SamsungSalesAnalyticsETL\").getOrCreate()\n",
        "\n",
        "spark"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 3) Helper Functions\n",
        "\n",
        "Utilities for cleaning, casting, and validating schemas."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def standardize_strings(df: DataFrame, cols: list[str]) -> DataFrame:\n",
        "    \"\"\"Trim + normalize casing for join keys and categorical values.\"\"\"\n",
        "    out = df\n",
        "    for c in cols:\n",
        "        out = out.withColumn(c, F.upper(F.trim(F.col(c))))\n",
        "    return out\n",
        "\n",
        "\n",
        "def safe_cast_numeric(df: DataFrame, col_name: str, to_type: T.DataType) -> DataFrame:\n",
        "    \"\"\"Cast a column to numeric safely; non-castable values become null.\"\"\"\n",
        "    return df.withColumn(col_name, F.col(col_name).cast(to_type))\n",
        "\n",
        "\n",
        "def assert_has_columns(df: DataFrame, required: list[str], df_name: str) -> None:\n",
        "    missing = [c for c in required if c not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"{df_name} missing required columns: {missing}. Found: {df.columns}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4) Extract\n",
        "\n",
        "Reads CSV inputs (header-based). For production, prefer explicit schemas over `inferSchema`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def read_sales_transactions(spark: SparkSession, path: str) -> DataFrame:\n",
        "    \"\"\"Expected columns: transaction_id, transaction_ts, store_id, sku, units, unit_price (+ optional model, currency).\"\"\"\n",
        "    return (\n",
        "        spark.read.format(\"csv\")\n",
        "        .option(\"header\", \"true\")\n",
        "        .option(\"inferSchema\", \"true\")\n",
        "        .load(f\"{path}/*.csv\")\n",
        "    )\n",
        "\n",
        "\n",
        "def read_product_catalog(spark: SparkSession, path: str) -> DataFrame:\n",
        "    \"\"\"Expected columns: sku, model (+ optional series, launch_year, msrp).\"\"\"\n",
        "    return (\n",
        "        spark.read.format(\"csv\")\n",
        "        .option(\"header\", \"true\")\n",
        "        .option(\"inferSchema\", \"true\")\n",
        "        .load(f\"{path}/*.csv\")\n",
        "    )\n",
        "\n",
        "\n",
        "def read_store_regions(spark: SparkSession, path: str) -> DataFrame:\n",
        "    \"\"\"Expected columns: store_id, country (+ optional store_name, city, region).\"\"\"\n",
        "    return (\n",
        "        spark.read.format(\"csv\")\n",
        "        .option(\"header\", \"true\")\n",
        "        .option(\"inferSchema\", \"true\")\n",
        "        .load(f\"{path}/*.csv\")\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5) Transform\n",
        "\n",
        "Cleans each dataset, enriches transactions via joins, and computes KPIs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def clean_sales(df: DataFrame) -> DataFrame:\n",
        "    required = [\"transaction_id\", \"transaction_ts\", \"store_id\", \"sku\", \"units\", \"unit_price\"]\n",
        "    assert_has_columns(df, required, \"sales_transactions\")\n",
        "\n",
        "    out = df\n",
        "    out = standardize_strings(out, [\"store_id\", \"sku\"])\n",
        "\n",
        "    if \"model\" in out.columns:\n",
        "        out = out.withColumn(\"model\", F.trim(F.col(\"model\")))\n",
        "\n",
        "    # timestamp parsing\n",
        "    out = out.withColumn(\n",
        "        \"transaction_ts\",\n",
        "        F.coalesce(\n",
        "            F.to_timestamp(\"transaction_ts\"),\n",
        "            F.to_timestamp(\"transaction_ts\", \"yyyy-MM-dd HH:mm:ss\"),\n",
        "            F.to_timestamp(\"transaction_ts\", \"MM/dd/yyyy HH:mm:ss\"),\n",
        "            F.to_timestamp(\"transaction_ts\", \"yyyy-MM-dd'T'HH:mm:ss\"),\n",
        "        ),\n",
        "    ).withColumn(\"transaction_date\", F.to_date(\"transaction_ts\"))\n",
        "\n",
        "    out = safe_cast_numeric(out, \"units\", T.IntegerType())\n",
        "    out = safe_cast_numeric(out, \"unit_price\", T.DoubleType())\n",
        "\n",
        "    # basic validity filters\n",
        "    out = out.filter(F.col(\"transaction_id\").isNotNull())\n",
        "    out = out.filter(F.col(\"sku\").isNotNull() & (F.col(\"sku\") != \"\"))\n",
        "    out = out.filter(F.col(\"store_id\").isNotNull() & (F.col(\"store_id\") != \"\"))\n",
        "    out = out.filter(F.col(\"units\").isNotNull() & (F.col(\"units\") > 0))\n",
        "    out = out.filter(F.col(\"unit_price\").isNotNull() & (F.col(\"unit_price\") >= 0))\n",
        "\n",
        "    if \"currency\" in out.columns:\n",
        "        out = out.withColumn(\"currency\", F.upper(F.trim(F.col(\"currency\"))))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def clean_products(df: DataFrame) -> DataFrame:\n",
        "    required = [\"sku\", \"model\"]\n",
        "    assert_has_columns(df, required, \"product_catalog\")\n",
        "\n",
        "    out = df\n",
        "    out = standardize_strings(out, [\"sku\"])\n",
        "    out = out.withColumn(\"model\", F.trim(F.col(\"model\")))\n",
        "\n",
        "    if \"msrp\" in out.columns:\n",
        "        out = safe_cast_numeric(out, \"msrp\", T.DoubleType())\n",
        "    if \"launch_year\" in out.columns:\n",
        "        out = safe_cast_numeric(out, \"launch_year\", T.IntegerType())\n",
        "\n",
        "    return out.dropDuplicates([\"sku\"])\n",
        "\n",
        "\n",
        "def clean_stores(df: DataFrame) -> DataFrame:\n",
        "    required = [\"store_id\", \"country\"]\n",
        "    assert_has_columns(df, required, \"store_regions\")\n",
        "\n",
        "    out = df\n",
        "    out = standardize_strings(out, [\"store_id\", \"country\"])\n",
        "    if \"region\" in out.columns:\n",
        "        out = out.withColumn(\"region\", F.trim(F.col(\"region\")))\n",
        "\n",
        "    return out.dropDuplicates([\"store_id\"])\n",
        "\n",
        "\n",
        "def enrich_transactions(sales: DataFrame, products: DataFrame, stores: DataFrame) -> DataFrame:\n",
        "    joined = (\n",
        "        sales.alias(\"s\")\n",
        "        .join(products.alias(\"p\"), on=F.col(\"s.sku\") == F.col(\"p.sku\"), how=\"left\")\n",
        "        .join(stores.alias(\"r\"), on=F.col(\"s.store_id\") == F.col(\"r.store_id\"), how=\"left\")\n",
        "    )\n",
        "\n",
        "    model_col = (\n",
        "        F.coalesce(F.col(\"s.model\"), F.col(\"p.model\"))\n",
        "        if \"model\" in sales.columns\n",
        "        else F.col(\"p.model\")\n",
        "    )\n",
        "\n",
        "    out = joined.select(\n",
        "        F.col(\"s.transaction_id\").alias(\"transaction_id\"),\n",
        "        F.col(\"s.transaction_ts\").alias(\"transaction_ts\"),\n",
        "        F.col(\"s.transaction_date\").alias(\"transaction_date\"),\n",
        "        F.col(\"s.store_id\").alias(\"store_id\"),\n",
        "        (F.col(\"r.store_name\").alias(\"store_name\") if \"store_name\" in stores.columns else F.lit(None).alias(\"store_name\")),\n",
        "        (F.col(\"r.city\").alias(\"city\") if \"city\" in stores.columns else F.lit(None).alias(\"city\")),\n",
        "        F.col(\"r.country\").alias(\"country\"),\n",
        "        (F.col(\"r.region\").alias(\"region\") if \"region\" in stores.columns else F.lit(None).alias(\"region\")),\n",
        "        F.col(\"s.sku\").alias(\"sku\"),\n",
        "        model_col.alias(\"model\"),\n",
        "        (F.col(\"p.series\").alias(\"series\") if \"series\" in products.columns else F.lit(None).alias(\"series\")),\n",
        "        (F.col(\"p.launch_year\").alias(\"launch_year\") if \"launch_year\" in products.columns else F.lit(None).alias(\"launch_year\")),\n",
        "        F.col(\"s.units\").alias(\"units\"),\n",
        "        F.col(\"s.unit_price\").alias(\"unit_price\"),\n",
        "        (F.col(\"s.units\") * F.col(\"s.unit_price\")).alias(\"line_revenue\"),\n",
        "        (F.col(\"s.currency\").alias(\"currency\") if \"currency\" in sales.columns else F.lit(None).alias(\"currency\")),\n",
        "    ).withColumn(\"model\", F.trim(F.col(\"model\")))\n",
        "\n",
        "    return out\n",
        "\n",
        "\n",
        "def compute_kpis(enriched: DataFrame) -> DataFrame:\n",
        "    return (\n",
        "        enriched\n",
        "        .filter(F.col(\"model\").isNotNull() & (F.col(\"model\") != \"\"))\n",
        "        .filter(F.col(\"country\").isNotNull() & (F.col(\"country\") != \"\"))\n",
        "        .groupBy(\"country\", \"model\")\n",
        "        .agg(\n",
        "            F.sum(\"units\").alias(\"units_sold\"),\n",
        "            F.round(F.sum(\"line_revenue\"), 2).alias(\"revenue\"),\n",
        "            F.countDistinct(\"transaction_id\").alias(\"transactions\"),\n",
        "            F.round(F.avg(\"unit_price\"), 2).alias(\"avg_unit_price\"),\n",
        "        )\n",
        "        .orderBy(F.col(\"revenue\").desc())\n",
        "    )\n",
        "\n",
        "\n",
        "def compute_summary(kpi_model_country: DataFrame) -> DataFrame:\n",
        "    return (\n",
        "        kpi_model_country\n",
        "        .groupBy(\"country\")\n",
        "        .agg(\n",
        "            F.sum(\"units_sold\").alias(\"units_sold\"),\n",
        "            F.round(F.sum(\"revenue\"), 2).alias(\"revenue\"),\n",
        "            F.sum(\"transactions\").alias(\"transactions\"),\n",
        "            F.countDistinct(\"model\").alias(\"distinct_models\"),\n",
        "        )\n",
        "        .orderBy(F.col(\"revenue\").desc())\n",
        "    )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6) Load (Write Parquet)\n",
        "\n",
        "Writes the enriched transactions and KPI outputs to Parquet."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def write_parquet(df: DataFrame, path: str, mode: str = \"overwrite\", partition_cols: list[str] | None = None) -> None:\n",
        "    writer = df.write.mode(mode).format(\"parquet\")\n",
        "    if partition_cols:\n",
        "        writer = writer.partitionBy(*partition_cols)\n",
        "    writer.save(path)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7) Run the Pipeline\n",
        "\n",
        "This executes the ETL end-to-end and shows a small preview of the KPI output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ---- Extract ----\n",
        "sales_raw = read_sales_transactions(spark, INPUT_SALES)\n",
        "products_raw = read_product_catalog(spark, INPUT_PRODUCTS)\n",
        "stores_raw = read_store_regions(spark, INPUT_STORES)\n",
        "\n",
        "# ---- Transform ----\n",
        "sales = clean_sales(sales_raw)\n",
        "products = clean_products(products_raw)\n",
        "stores = clean_stores(stores_raw)\n",
        "\n",
        "enriched = enrich_transactions(sales, products, stores)\n",
        "kpi_model_country = compute_kpis(enriched)\n",
        "kpi_summary = compute_summary(kpi_model_country)\n",
        "\n",
        "# ---- Load ----\n",
        "write_parquet(enriched, OUT_ENRICHED, mode=WRITE_MODE, partition_cols=[\"transaction_date\"])\n",
        "write_parquet(kpi_model_country, OUT_KPI_MODEL_COUNTRY, mode=WRITE_MODE)\n",
        "write_parquet(kpi_summary, OUT_KPI_SUMMARY, mode=WRITE_MODE)\n",
        "\n",
        "print(f\"âœ… Enriched transactions written to: {OUT_ENRICHED}\")\n",
        "print(f\"âœ… KPI (model/country) written to: {OUT_KPI_MODEL_COUNTRY}\")\n",
        "print(f\"âœ… KPI summary written to: {OUT_KPI_SUMMARY}\")\n",
        "\n",
        "display(kpi_model_country.limit(20)) if 'display' in globals() else kpi_model_country.show(20, truncate=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8) Notes / Next Enhancements\n",
        "\n",
        "- Add a **quarantine** output for bad rows (DQ layer)\n",
        "- Add **incremental processing** with a watermark on `transaction_ts`\n",
        "- Add more marts: top models by region, ASP by series, MoM trends\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.x"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}